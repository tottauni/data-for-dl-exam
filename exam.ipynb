{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQHqEcfGnrxU"
   },
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 16950,
     "status": "ok",
     "timestamp": 1737986623620,
     "user": {
      "displayName": "Carlotta de' Castiglioni",
      "userId": "17124115086511107324"
     },
     "user_tz": -60
    },
    "id": "q4ySgTDenq3K"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.activations import softmax\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import product \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5gdG_sdmTtr"
   },
   "source": [
    "## PREPARING THE INPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[point 2 in the exam text: \\\n",
    "*For simplicity, but not necessary) and to make my data more understandable (to me), i would start transforming my vector in a matrix 33x4, where the rows correspond to the 33 sidepoints, and the columns are x, y, z, v. I am going to apply a minmax scaling: considering (0.493, 0.631) for the X column, and so on... \\\n",
    "The classes are more or less balanced, so for the moment I would not apply any upsampling/downsampling for the minority/majority classes. I could try it only if the performance of my model will not satisfy me. \\\n",
    "I will also transform my outputs from strings to integers (from 0 to 6).*]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the data and we look at their shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 592,
     "status": "ok",
     "timestamp": 1737978679895,
     "user": {
      "displayName": "Carlotta de' Castiglioni",
      "userId": "17124115086511107324"
     },
     "user_tz": -60
    },
    "id": "iPowka6xmI-h"
   },
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\yoshi\\OneDrive\\Desktop\\exam deep learning\\input_data.pkl'\n",
    "with open(file_path, \"rb\") as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1737978830216,
     "user": {
      "displayName": "Carlotta de' Castiglioni",
      "userId": "17124115086511107324"
     },
     "user_tz": -60
    },
    "id": "W3NR06ISmeVr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data for key 'X': [[ 0.57770568  0.23191446 -0.4204722  ...  0.95409644  0.14137968\n",
      "   0.97238696]\n",
      " [ 0.57770413  0.23211703 -0.4230628  ...  0.95341611  0.14557168\n",
      "   0.97199601]\n",
      " [ 0.5777995   0.2321346  -0.42221451 ...  0.95276618  0.1410574\n",
      "   0.97163814]\n",
      " ...\n",
      " [ 0.61882263  0.23206101 -0.39901862 ...  0.96600813  0.04200708\n",
      "   0.91995174]\n",
      " [ 0.61883748  0.23205066 -0.40318117 ...  0.96604252  0.04207094\n",
      "   0.91996664]\n",
      " [ 0.61884356  0.23205319 -0.40282306 ...  0.96613091  0.03997473\n",
      "   0.92123002]]\n"
     ]
    }
   ],
   "source": [
    "sample_key = list(data.keys())[0]\n",
    "print(f\"Sample data for key '{sample_key}':\", data[sample_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And divide the sample from theit classification output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (2700, 132)\n",
      "Shape of y: (2700,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(data['X'])\n",
    "y = np.array(data['y'])\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can transform from vector to matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1737978701121,
     "user": {
      "displayName": "Carlotta de' Castiglioni",
      "userId": "17124115086511107324"
     },
     "user_tz": -60
    },
    "id": "p0ZLeANBmfXj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of reshaped X: (2700, 33, 4)\n"
     ]
    }
   ],
   "source": [
    "X_reshaped = X.reshape(-1, 33, 4)\n",
    "\n",
    "print(\"Shape of reshaped X:\", X_reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check the first sample to see how it looks now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First reshaped sample (33x4 matrix):\n",
      "[[ 0.57770568  0.23191446 -0.4204722   0.9999938 ]\n",
      " [ 0.58672541  0.2164979  -0.40176901  0.99997592]\n",
      " [ 0.59186375  0.2168406  -0.40179992  0.99997413]\n",
      " [ 0.5957647   0.21739218 -0.40178603  0.99997211]\n",
      " [ 0.57161874  0.21834612 -0.40327197  0.9999702 ]\n",
      " [ 0.56672728  0.21924257 -0.40329617  0.99996722]\n",
      " [ 0.56239396  0.22022381 -0.40333375  0.99996448]\n",
      " [ 0.60183752  0.22745022 -0.2632179   0.9999491 ]\n",
      " [ 0.55938202  0.22777343 -0.27078733  0.99993861]\n",
      " [ 0.58797812  0.25179648 -0.3664923   0.99999428]\n",
      " [ 0.56935519  0.25208199 -0.36876363  0.99999261]\n",
      " [ 0.63961399  0.33558801 -0.15272155  0.99998927]\n",
      " [ 0.52481461  0.33422393 -0.18341279  0.9999733 ]\n",
      " [ 0.64599973  0.46432573 -0.08530901  0.9896456 ]\n",
      " [ 0.51587093  0.46985579 -0.13175203  0.98091239]\n",
      " [ 0.64039892  0.58820516 -0.21137002  0.98633075]\n",
      " [ 0.50740278  0.58851296 -0.24127439  0.96922094]\n",
      " [ 0.63988477  0.62180054 -0.24995236  0.97064441]\n",
      " [ 0.5041073   0.62379986 -0.28097337  0.94724053]\n",
      " [ 0.6320051   0.62147838 -0.2905513   0.97263587]\n",
      " [ 0.51203376  0.62342221 -0.32364249  0.95022303]\n",
      " [ 0.62947905  0.61173481 -0.22928771  0.96724874]\n",
      " [ 0.51610702  0.61270922 -0.25997823  0.94162697]\n",
      " [ 0.60403955  0.5764879   0.0130182   0.99986577]\n",
      " [ 0.54251695  0.57375395 -0.01314482  0.99980384]\n",
      " [ 0.5967716   0.74925727  0.02240757  0.98229247]\n",
      " [ 0.5426833   0.7455827   0.01337811  0.98515981]\n",
      " [ 0.58853406  0.90164685  0.2995322   0.97869062]\n",
      " [ 0.55103886  0.89875692  0.27710819  0.97898656]\n",
      " [ 0.58216023  0.9150272   0.31853583  0.88188869]\n",
      " [ 0.55297643  0.91358781  0.29569319  0.81360596]\n",
      " [ 0.60081506  0.95406067  0.17002827  0.97483033]\n",
      " [ 0.54984224  0.95409644  0.14137968  0.97238696]]\n"
     ]
    }
   ],
   "source": [
    "print(\"First reshaped sample (33x4 matrix):\")\n",
    "print(X_reshaped[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply minmax scaling for the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1737978725010,
     "user": {
      "displayName": "Carlotta de' Castiglioni",
      "userId": "17124115086511107324"
     },
     "user_tz": -60
    },
    "id": "Gndu0X4ZmmKR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data shape: (2700, 33, 4)\n"
     ]
    }
   ],
   "source": [
    "# initialize minmax scaler for each coorinate + visibility\n",
    "scalers = [MinMaxScaler() for _ in range(4)]\n",
    "\n",
    "# create an empty array to store the scaled data\n",
    "X_scaled = np.zeros_like(X_reshaped)\n",
    "\n",
    "# apply minmax scaler independently to each column\n",
    "for i in range(4):\n",
    "    feature_data = X_reshaped[:, i, :] # identify the specific column\n",
    "    X_scaled[:, i, :] = scalers[i].fit_transform(feature_data)\n",
    "\n",
    "# check the shape\n",
    "print(\"Scaled data shape:\", X_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we consider the outputs. First, we check how they look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes and their counts:\n",
      "left_bicep: 435\n",
      "left_shoulder: 373\n",
      "left_tricep: 317\n",
      "rest: 406\n",
      "right_bicep: 369\n",
      "right_shoulder: 401\n",
      "right_tricep: 399\n"
     ]
    }
   ],
   "source": [
    "unique_classes, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "print(\"Unique classes and their counts:\")\n",
    "for class_label, count in zip(unique_classes, counts):\n",
    "    print(f\"{class_label}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a labelencoder, we transform the strings to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kN7vqiNBmr_x"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_encoded = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we print the conversion, to keep track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class to Encoded Mapping:\n",
      "left_bicep -> 0\n",
      "left_shoulder -> 1\n",
      "left_tricep -> 2\n",
      "rest -> 3\n",
      "right_bicep -> 4\n",
      "right_shoulder -> 5\n",
      "right_tricep -> 6\n",
      "\n",
      "Unique classes and their counts:\n",
      "0: 435\n",
      "1: 373\n",
      "2: 317\n",
      "3: 406\n",
      "4: 369\n",
      "5: 401\n",
      "6: 399\n"
     ]
    }
   ],
   "source": [
    "label_mapping = {original: encoded for original, encoded in zip(label_encoder.classes_, range(len(label_encoder.classes_)))}\n",
    "print(\"\\nClass to Encoded Mapping:\")\n",
    "for original_label, encoded_label in label_mapping.items():\n",
    "    print(f\"{original_label} -> {encoded_label}\")\n",
    "print()\n",
    "\n",
    "# check if it worked (confront to the print before)\n",
    "unique_classes, counts = np.unique(y_encoded, return_counts=True)\n",
    "print(\"Unique classes and their counts:\")\n",
    "for class_label, count in zip(unique_classes, counts):\n",
    "    print(f\"{class_label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename to make order\n",
    "\n",
    "final_x = X_scaled\n",
    "final_y = y_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fk9mdUUm0k3"
   },
   "source": [
    "## CREATE THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[point 1 in the exam text: \\\n",
    "\\\n",
    "*For this kind of task, I would use a FCN, because it works good for tabular data (like mine), and it is able to capture both the local and the global features. It also allows me to use fully connected layers, that are exactly what I need in my task*]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[point 5 in the exam text: \\\n",
    "\\\n",
    "*MODEL COMPOSITION* \\\n",
    "*input (preprocessed) -> fully connected layer -> dropout -> fully connected layer -> dropout -> output* \\\n",
    "\\\n",
    "*ACTIVATION FUNCTIONS* \\\n",
    "*ReLu inside hidden layers* \\\n",
    "*softmax for output layer* \\\n",
    "\\\n",
    "*HYPERPARAMETERS* \\\n",
    "*I would try different numbers of hidden layers, learning rate and dropout rate.* \\\n",
    "*Also, I would start with a certain number of epochs and check the permormance during the epochs. If the performance is getting worse after a certain number of epochs, I reduce it.* ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[point 3 in the exam: \\\n",
    "\\\n",
    "*The output is going to be a fully connected layer with softmax activation that returns a vector of length 7 (corresponding to the classes). This is because, doing that, I have all my values corresponding to a probability distribution (they sum up to 1). The biggest value position corresponds to the predicted class.* ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[point 4 in the exam: \\\n",
    "\\\n",
    "*For this task, I am going to use categorical cross-entropy. Since I am dealing with multi-class classification, I need my error to be evaluated as the \"distance\" of my predicted class to the true class of the sample. The farest the prediction is from the reality, the bigger the error calculated.* ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**CHANGE FROM EXAM TEXT**</font> \\\n",
    "I had to add a flatten layer that I forgot to write in the exam text. Since I reshaped my data in the preprocessing, and transformed them from vector (132,) to matrix 33x4, now (after the scaling) I need to go back to the vector to feed them in the fcn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4GYxMv6Vm6Rd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">17,024</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">455</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m132\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m17,024\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                   │             \u001b[38;5;34m455\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,735</span> (100.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,735\u001b[0m (100.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,735</span> (100.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,735\u001b[0m (100.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_model(input_shape=(33, 4), num_classes=7, nhid1=128, nhid2=64, learning_rate=0.001, dropout_rate=0.3, hid_act='relu', loss='sparse_categorical_crossentropy'):\n",
    "    model = Sequential()\n",
    "\n",
    "    # input layer\n",
    "    model.add(Input(shape=input_shape))\n",
    "    \n",
    "    # flatten layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # first hidden layer\n",
    "    model.add(Dense(nhid1, activation=hid_act))\n",
    "    model.add(Dropout(dropout_rate))  # regularization\n",
    "    \n",
    "    # second hidden layer\n",
    "    model.add(Dense(nhid2, activation=hid_act))\n",
    "    model.add(Dropout(dropout_rate))  # regularization\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # compile\n",
    "    model.compile(  \n",
    "        loss=loss, \n",
    "        metrics=['f1_score']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "# print the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0CREI70pRpY"
   },
   "source": [
    "**Hyperparametrization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'model__nhid1': [75, 100, 125],\n",
    "    'model__nhid2': [50, 75],\n",
    "    'model__learning_rate': [0.2, 0.1],\n",
    "    'model__dropout_rate': [0, 0.2],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[point 6 in the exam: \\\n",
    "*First, I would split my data between train and test (80%, 20% seems a reasonable number)* \\\n",
    "*[...]* \\\n",
    "*I would evaluate the performance with F1.* ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    final_x, final_y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best hyperparameters, I firs generate all the possible combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in product(*values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I start doing the hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 50, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.791670099668113\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 50, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8340249228724211\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 50, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.7111629405025985\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 50, 'model__learning_rate': 0.001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.864602585646317\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 50, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.7703806862627657\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 50, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8002154131553718\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 50, 'model__learning_rate': 0.01, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.7883892937715594\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 50, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8156720247731822\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 50, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8318590166034217\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 75, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.7942005723806354\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 75, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.822883692132201\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 75, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.7951048624042322\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 75, 'model__learning_rate': 0.001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8385926854955019\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 75, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8494519271413367\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 75, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.7972124001499993\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 75, 'model__learning_rate': 0.01, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8470635200892871\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 75, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8354698197374267\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 75, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.6985402020961221\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 100, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8647744779924394\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 100, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8379246415339474\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 100, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.665266280747075\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 100, 'model__learning_rate': 0.001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.7760663111443976\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 100, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8347588947662966\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 100, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.857025450564987\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 100, 'model__learning_rate': 0.01, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.742391492146912\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 100, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.7834715226801777\n",
      "Testing parameters: {'model__nhid1': 75, 'model__nhid2': 100, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8470729939415207\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 50, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8503165302867676\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 50, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.7518166188141532\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 50, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.7982911297408531\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 50, 'model__learning_rate': 0.001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8621463846256323\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 50, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8372973075843804\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 50, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.743761650562194\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 50, 'model__learning_rate': 0.01, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.7956235266507616\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 50, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8599969012776731\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 50, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8740356147943773\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 75, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.7227877789706885\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 75, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8468215190229686\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 75, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8619628185311016\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 75, 'model__learning_rate': 0.001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.7610015705442955\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 75, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8593096960916222\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 75, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8290774659543471\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 75, 'model__learning_rate': 0.01, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.7631073293291463\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 75, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8330530374422694\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 75, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8435853024070793\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 100, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8986845773599492\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 100, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8831305118080719\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 100, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8264670251040558\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 100, 'model__learning_rate': 0.001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.7962306155434847\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 100, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8941541626986487\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 100, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8383743322863705\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 100, 'model__learning_rate': 0.01, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.888078164012742\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 100, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.767857822313778\n",
      "Testing parameters: {'model__nhid1': 100, 'model__nhid2': 100, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8386670787805458\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 50, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.7211459594250801\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 50, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.73657645180402\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 50, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.852299182287704\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 50, 'model__learning_rate': 0.001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8288189095409998\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 50, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8838712004888196\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 50, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8677711658487973\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 50, 'model__learning_rate': 0.01, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8095424774559499\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 50, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.7633205387530417\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 50, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.7693632496434394\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 75, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.7154351616306949\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 75, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8044035958138827\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 75, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8079163809562511\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 75, 'model__learning_rate': 0.001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8426376175518271\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 75, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8719184674445193\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 75, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.7329453668428415\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 75, 'model__learning_rate': 0.01, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8584164370709202\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 75, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8532957586749067\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 75, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.7652295903753413\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 100, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8486066977254535\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 100, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.779692421118139\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 100, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.7233645036731804\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 100, 'model__learning_rate': 0.001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8607631349324205\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 100, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.832903095395791\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 100, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8589675778196458\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 100, 'model__learning_rate': 0.01, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8243671671451696\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 100, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8248762055553972\n",
      "Testing parameters: {'model__nhid1': 125, 'model__nhid2': 100, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.767308757050492\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 50, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8277117784742264\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 50, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.7759851596654207\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 50, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8420030881341498\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 50, 'model__learning_rate': 0.001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.7899974624089986\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 50, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8394696158758873\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 50, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8552130848108068\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 50, 'model__learning_rate': 0.01, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8409396309136111\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 50, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8862963962809948\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 50, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.814110494726012\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 75, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.7559086566328681\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 75, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8537163386809832\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 75, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8925694479288159\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 75, 'model__learning_rate': 0.001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.7536353079703424\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 75, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.821870370313948\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 75, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8420504896376414\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 75, 'model__learning_rate': 0.01, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8268595455996303\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 75, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8520650874684854\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 75, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8824274690489047\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 100, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8839943881550436\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 100, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8245039015419466\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 100, 'model__learning_rate': 0.0001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8340246820377938\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 100, 'model__learning_rate': 0.001, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8740837895361868\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 100, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.8071083439130184\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 100, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8742641741323298\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 100, 'model__learning_rate': 0.01, 'model__dropout_rate': 0}\n",
      "F1 Score: 0.8676278047667715\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 100, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.2}\n",
      "F1 Score: 0.7973189152988139\n",
      "Testing parameters: {'model__nhid1': 150, 'model__nhid2': 100, 'model__learning_rate': 0.01, 'model__dropout_rate': 0.4}\n",
      "F1 Score: 0.8732548281089865\n",
      "CPU times: total: 3min 56s\n",
      "Wall time: 4h 49min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# keep track of the results\n",
    "best_params = None\n",
    "best_f1 = 0\n",
    "\n",
    "# loop through each hyperparameter combination\n",
    "for params in param_combinations:\n",
    "    print(f\"Testing parameters: {params}\")\n",
    "    \n",
    "    # create and train the model with current hyperparameters\n",
    "    model = KerasClassifier(\n",
    "        model=create_model,\n",
    "        model__input_shape=(33, 4),\n",
    "        model__num_classes=7,\n",
    "        model__nhid1=params['model__nhid1'],\n",
    "        model__nhid2=params['model__nhid2'],\n",
    "        model__learning_rate=params['model__learning_rate'],\n",
    "        model__dropout_rate=params['model__dropout_rate'],\n",
    "        batch_size=32,\n",
    "        epochs=5,  # PUT TO 50\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test data\n",
    "    pred = model.predict(X_test)\n",
    "    \n",
    "    # evaluate model performance\n",
    "    f1 = f1_score(y_test, pred, average='weighted')\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    \n",
    "    # Store best hyperparameters\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Best Results ====================\n",
      "Best Hyperparameters:\n",
      " - Number of hidden units in Layer 1: 150\n",
      " - Number of hidden units in Layer 2: 75\n",
      " - Learning Rate: 0.001\n",
      " - Dropout Rate: 0.4\n",
      "\n",
      "Performance Metrics:\n",
      " - Best F1 Score: 0.9075\n",
      "========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==================== Best Results ====================\")\n",
    "print(f\"Best Hyperparameters:\")\n",
    "print(f\" - Number of hidden units in Layer 1: {best_params['model__nhid1']}\")\n",
    "print(f\" - Number of hidden units in Layer 2: {best_params['model__nhid2']}\")\n",
    "print(f\" - Learning Rate: {best_params['model__learning_rate']}\")\n",
    "print(f\" - Dropout Rate: {best_params['model__dropout_rate']}\")\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\" - Best F1 Score: {best_f1:.4f}\")\n",
    "print(\"========================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the best parameter\n",
    "\n",
    "final_params = best_params.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV-GrrdFpyfN"
   },
   "source": [
    "## MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[point 6 in the exam: \\\n",
    "*Within the train set, I can divide again using validation sets, and I can do that with a cross validation procedure.* \\\n",
    "*Then, after the source for the best hyperparameters configuration, I would evaluate the performance on the test set of the best configuration, using f1.* ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store the results\n",
    "cv_f1_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I create the model with the best hyperparameters that I found before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(\n",
    "    model=create_model,\n",
    "    model__input_shape=(33, 4),\n",
    "    model__num_classes=7,\n",
    "    **final_params,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "    # split into training and validation\n",
    "    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    # train the model\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # predict on the validation fold\n",
    "    pred = model.predict(X_val_fold)\n",
    "\n",
    "    # evaluate model performance\n",
    "    f1 = f1_score(y_val_fold, pred, average='weighted')\n",
    "\n",
    "    # store the results\n",
    "    cv_f1_scores.append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the results we obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cross-Validation F1 Scores ---\n",
      "Fold   F1 Score       \n",
      "------------------------------\n",
      "1      0.7302\n",
      "2      0.7653\n",
      "3      0.8389\n",
      "4      0.7616\n",
      "5      0.7707\n",
      "\n",
      "--- Cross-Validation Summary ---\n",
      "Mean F1 Score: 0.7733\n",
      "Standard Deviation: 0.0357\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Cross-Validation F1 Scores ---\")\n",
    "print(f\"{'Fold':<6} {'F1 Score':<15}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for fold, f1_score in enumerate(cv_f1_scores, 1):\n",
    "    print(f\"{fold:<6} {f1_score:.4f}\")\n",
    "\n",
    "mean_f1 = np.mean(cv_f1_scores)\n",
    "std_f1 = np.std(cv_f1_scores)\n",
    "\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "print(f\"Mean F1 Score: {mean_f1:.4f}\")\n",
    "print(f\"Standard Deviation: {std_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONSIDERATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My F1 score dropped during cross-validation. This could be due to the model being trained on smaller, potentially less representative subsets of the data in each fold, which can result in slightly lower performance compared to training on the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dince the main focus of this project was not on achieving optimal performance, I did not analyze the performance across epochs to determine whether reducing the number of epochs would have been beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP0MPuZzdSh58juMMNEYETR",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
